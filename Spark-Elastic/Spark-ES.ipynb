{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming para Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Conexão do Elasticsearch por padrão, nos conectamos ao elasticsearch: 9200, como estamos executando este notebook \n",
    "# no Spark-Node, precisamos usar 'elasticsearch-node' em vez de 'localhost', pois esse é o nome do docker container\n",
    "# executando o Elasticsearch. Se o índice de teste de fluxo existir, limpe-o e crie um novo.\n",
    "\n",
    "es = Elasticsearch('elasticsearch-node:9200')\n",
    "\n",
    "if es.indices.exists('stream-test'):\n",
    "    es.indices.delete('stream-test')\n",
    "    \n",
    "    body={\n",
    "        'mappings': {\n",
    "            'properties': {\n",
    "                'count': {'type': 'text'},\n",
    "                'name': {'type': 'text'},\n",
    "                'value': {'type': 'text'},\n",
    "                'timestamp': {'type': 'text'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    es.indices.create(index='stream-test', body=body)\n",
    "    \n",
    "print(es.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precisamos garantir que o conector ES-Hadoop esteja no caminho de classe do driver; \n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars elasticsearch-hadoop-7.5.2/dist/elasticsearch-spark-20_2.11-7.5.2.jar pyspark-shell'\n",
    "\n",
    "# SparkContext -- É o portão de entrada da funcionalidade Apache Spark.\n",
    "\n",
    "sc = SparkContext(appName=\"PythonSparkStreaming\")\n",
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StreamingContext -- É o ponto de entrada para todas as funcionalidades do Spark Streaming.\n",
    "# (representa a conexão com um cluster Spark e pode ser usado para criar várias fontes de entrada).\n",
    "\n",
    "ssc = StreamingContext(sc, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1. Transmitiremos todos os arquivos gravados no diretório de amostra. Isso está sendo bombeado com dados aleatórios.\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "json_folder_path = ('./sample/')\n",
    "\n",
    "json_files = [ x for x in os.listdir(json_folder_path ) if x.endswith(\"json\") ]\n",
    "\n",
    "for json_file in json_files:\n",
    "\n",
    "    json_file_path = os.path.join(json_folder_path, json_file)\n",
    "\n",
    "    with open (json_file_path) as f:\n",
    "\n",
    "        data_dict = json.loads(f.read())  \n",
    "\n",
    "        data_dict['count'] = data_dict.pop('count')\n",
    "        \n",
    "        print(data_dict['count'])\n",
    "        \n",
    "        stream = ssc.textFileStream(json_file_path)\n",
    "\n",
    "# 2. Gravar o RDD gerado por cada operação em lote de streaming no Elasticsearch.\n",
    "\n",
    "        es_write_conf = {\n",
    "            \"es.nodes\" : '927d8d22d8c7',\n",
    "            \"es.port\" : '9200',\n",
    "            \"es.resource\" : 'stream-test',\n",
    "            \"es.input.json\" : \"yes\",\n",
    "            \"es.mapping.id\": \"count\"\n",
    "        }\n",
    "    \n",
    "# RDD -- Conjuntos de dados distribuídos resilientes (RDD) é uma estrutura de dados fundamental do Spark. \n",
    "#        É uma coleção imutável de objetos distribuídos. Cada conjunto de dados no RDD é dividido em partições lógicas,\n",
    "#        que podem ser computadas em diferentes nós do cluster.\n",
    "\n",
    "# PARALLELIZE -- É uma estrutura de dados fundamental do Spark, é uma coleção imutável de objetos distribuídos. \n",
    "#                Cada conjunto de dados no RDD é dividido em partições lógicas, que podem ser computadas em diferentes\n",
    "#                nós do cluster\n",
    "\n",
    "        rdd = sc.parallelize(data_dict)\n",
    "        \n",
    "        def format_data(x):\n",
    "            return (data_dict['count'], json.dumps(data_dict))      \n",
    "\n",
    "        \n",
    "# LAMBDA -- Uma função lambda pode receber qualquer número de argumentos, mas pode ter apenas uma expressão.\n",
    "#           Também conhecidas como funções anônimas, são pequenas funções restritas que não precisam de um \n",
    "#           nome (ou seja, um identificador).\n",
    "\n",
    "# MAP -- A operação Map() se aplica a cada elemento do RDD e retorna o resultado como novo RDD. \n",
    "#        No mapa, o desenvolvedor pode definir sua própria lógica de negócios customizada. \n",
    "#        A mesma lógica será aplicada a todos os elementos do RDD.\n",
    "\n",
    "\n",
    "        rdd = rdd.map(lambda x: format_data(x))\n",
    "        \n",
    "        parsed = stream.map(lambda x: format_data(x))\n",
    "        \n",
    "# saveAsNewAPIHadoopFile -- Método para salvar o RDD no ElasticSearch.  \n",
    "\n",
    "        rdd.saveAsNewAPIHadoopFile(\n",
    "            path='-',\n",
    "            outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n",
    "            keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "            valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "            conf=es_write_conf\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# foreachRDD --É um \"operador de saída\" no Spark Streaming. Ele permite que você acesse os RDDs subjacentes \n",
    "# do DStream para executar ações que fazem algo prático com os dados. Por exemplo, usando o foreachRDD, \n",
    "# você pode gravar dados em um banco de dados.\n",
    "\n",
    "parsed.foreachRDD(lambda rdd: handler(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solicitar que também seja impresso em 'stdout'.\n",
    "\n",
    "parsed.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Começar o contexto do spark.\n",
    "\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parar o contexto.\n",
    "\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulk Processing ES with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"PythonSparkReading\")  \n",
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_read_conf = {\n",
    "    \"es.nodes\" : '927d8d22d8c7',\n",
    "    \"es.port\" : '9200',\n",
    "    \"es.resource\" : 'stream-test'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenha um RDD para um determinado arquivo Hadoop com uma nova API InputFormat arbitrária e \n",
    "# opções de configuração extras para passar para o formato de entrada.\n",
    "\n",
    "es_rdd = sc.newAPIHadoopRDD(\n",
    "    inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "    conf=es_read_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retornar os 5 primeiros documentos, numa lista com id(count) e o resto da informação.\n",
    "\n",
    "es_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_rdd = es_rdd.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retornar um unico elemento, numa lista com todas as suas informações.\n",
    "\n",
    "es_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark SQL\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = es_rdd.map(lambda l: Row(**dict(l))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df \\\n",
    "    .groupby('value') \\\n",
    "    .count() \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df \\\n",
    "    .filter(df.name == 'Legolas')\\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
