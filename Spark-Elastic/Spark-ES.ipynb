{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming para Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conexão do Elasticsearch por padrão, nos conectamos ao elasticsearch: 9200, como estamos executando este notebook no Spark-Node, precisamos usar 'elasticsearch-node' em vez de 'localhost', pois esse é o nome do docker container executando o Elasticsearch. Se o índice de teste de fluxo existir, limpe-o e crie um novo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "es = Elasticsearch('elasticsearch-node:9200')\n",
    "\n",
    "if es.indices.exists('stream-test'):\n",
    "    es.indices.delete('stream-test')\n",
    "    \n",
    "    body={\n",
    "        'mappings': {\n",
    "            'properties': {\n",
    "                'count': {'type': 'text'},\n",
    "                'name': {'type': 'text'},\n",
    "                'value': {'type': 'text'},\n",
    "                'timestamp': {'type': 'text'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    es.indices.create(index='stream-test', body=body)\n",
    "    \n",
    "print(es.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precisamos garantir que o conector ES-Hadoop esteja no caminho de classe do driver; \n",
    "#### SparkContext -- É o portão de entrada da funcionalidade Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars elasticsearch-hadoop-7.5.2/dist/elasticsearch-spark-20_2.11-7.5.2.jar pyspark-shell'\n",
    "\n",
    "sc = SparkContext(appName=\"PythonSparkStreaming\")\n",
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StreamingContext -- É o ponto de entrada para todas as funcionalidades do Spark Streaming (representa a conexão com um cluster Spark e pode ser usado para criar várias fontes de entrada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ssc = StreamingContext(sc, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transmitiremos todos os arquivos gravados no diretório de amostra. Isso está sendo bombeado com dados aleatórios. Gravar o RDD gerado por cada operação em lote de streaming no Elasticsearch.\n",
    "\n",
    "#### textFileStream -- Procura dentro da pasta indicada e lê esses dados no DStreams sem considerar nenhum tipo de formato.\n",
    "#### RDD -- Conjuntos de dados distribuídos resilientes (RDD) é uma estrutura de dados fundamental do Spark. É uma coleção imutável de objetos distribuídos. Cada conjunto de dados no RDD é dividido em partições lógicas, que podem ser computadas em diferentes nós do cluster.\n",
    "#### PARALLELIZE -- É uma estrutura de dados fundamental do Spark, é uma coleção imutável de objetos distribuídos. Cada conjunto de dados no RDD é dividido em partições lógicas, que podem ser computadas em diferentes nós do cluster.\n",
    "#### LAMBDA -- Uma função lambda pode receber qualquer número de argumentos, mas pode ter apenas uma expressão. Também conhecidas como funções anônimas, são pequenas funções restritas que não precisam de um nome (ou seja, um identificador).\n",
    "#### MAP -- A operação Map() se aplica a cada elemento do RDD e retorna o resultado como novo RDD. No mapa, o desenvolvedor pode definir sua própria lógica de negócios customizada. A mesma lógica será aplicada a todos os elementos do RDD.\n",
    "#### saveAsNewAPIHadoopFile -- Método para salvar o RDD no ElasticSearch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "#stream = ssc.textFileStream('./sample/')\n",
    "#\n",
    "#def format_sample(x):\n",
    "#    data = json.loads(x)\n",
    "#    data['timestamp'] = datetime.fromtimestamp(data['timestamp']).strftime('%Y/%m/%d %H:%M:%S')\n",
    "#    data['doc_id'] = data.pop('count')\n",
    "#    return (data['doc_id'], json.dumps(data))\n",
    "#\n",
    "#parsed = stream.map(lambda x: format_sample(x))\n",
    "#\n",
    "#def handler(rdd):\n",
    "#        es_write_conf = {       \n",
    "#        \"es.nodes\" : '15369fadfcbe',\n",
    "#        \"es.port\" : '9200',\n",
    "#        \"es.resource\" : 'stream-test',\n",
    "#        \"es.input.json\" : \"true\",\n",
    "#        \"mapred.reduce.tasks.speculative.execution\": \"false\",\n",
    "#        \"mapred.map.tasks.speculative.execution\": \"false\",\n",
    "#        \"es.mapping.id\": \"doc_id\",\n",
    "#        }\n",
    "#\n",
    "#        rdd.saveAsNewAPIHadoopFile(\n",
    "#                path='-',\n",
    "#                outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n",
    "#                keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "#                valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "#                conf=es_write_conf)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "json_folder_path = ('./sample/')\n",
    "\n",
    "json_files = [ x for x in os.listdir(json_folder_path ) if x.endswith(\"json\") ]\n",
    "\n",
    "for json_file in json_files:\n",
    "\n",
    "    json_file_path = os.path.join(json_folder_path, json_file)\n",
    "\n",
    "    with open (json_file_path) as f:\n",
    "\n",
    "        data_dict = json.loads(f.read())  \n",
    "\n",
    "        data_dict['count'] = data_dict.pop('count')\n",
    "        \n",
    "        print(data_dict)\n",
    "        \n",
    "        es_write_conf = {\n",
    "            \"es.nodes\" : '15369fadfcbe',\n",
    "            #\"es.nodes\" : '927d8d22d8c7',\n",
    "            \"es.port\" : '9200',\n",
    "            \"es.resource\" : 'stream-test',\n",
    "            \"es.input.json\" : \"yes\",\n",
    "            \"es.mapping.id\": \"count\"\n",
    "        }\n",
    "\n",
    "        rdd = sc.parallelize(data_dict)\n",
    "        \n",
    "        def format_data(x):\n",
    "            return (data_dict['count'], json.dumps(data_dict))      \n",
    "\n",
    "        rdd = rdd.map(lambda x: format_data(x))\n",
    "                \n",
    "        rdd.saveAsNewAPIHadoopFile(\n",
    "            path='-',\n",
    "            outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n",
    "            keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "            valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "            conf=es_write_conf\n",
    "        )\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### foreachRDD --É um \"operador de saída\" no Spark Streaming. Ele permite que você acesse os RDDs subjacentes do DStream para executar ações que fazem algo prático com os dados. Por exemplo, usando o foreachRDD, você pode gravar dados em um banco de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#parsed.foreachRDD(lambda rdd: handler(rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solicitar que também seja impresso em 'stdout'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parsed.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Começar o contexto do spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parar o contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulk Processing ES with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "sc = SparkContext(appName=\"PythonSparkReading\")  \n",
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_read_conf = {\n",
    "    \"es.nodes\" : '15369fadfcbe',\n",
    "    #\"es.nodes\" : '927d8d22d8c7',\n",
    "    \"es.port\" : '9200',\n",
    "    \"es.resource\" : 'stream-test',\n",
    "    \"es.mapping.id\": \"doc_id\",\n",
    "    \"es.mapping.id\": \"count\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtenha um RDD para um determinado arquivo Hadoop com uma nova API InputFormat arbitrária e opções de configuração extras para passar para o formato de entrada.\n",
    "#### newAPIHadoopRDD -- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_rdd = sc.newAPIHadoopRDD(\n",
    "    inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "    conf=es_read_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retornar os 5 primeiros documentos, numa lista com id(count) e o resto da informação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "es_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos converter essas tuplas em JSON puro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "es_rdd = es_rdd.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retornar um unico elemento, numa lista com todas as suas informações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converter o RDD em um Dataframe Spark SQL para que possamos tratá-lo mais como um objeto Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark SQL\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = es_rdd.map(lambda l: Row(**dict(l))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df \\\n",
    "    .groupby('value') \\\n",
    "    .count() \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df \\\n",
    "    .filter(df.name == 'Legolas')\\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
