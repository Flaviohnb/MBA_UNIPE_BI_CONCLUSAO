{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Spark Streaming para Elasticsearch</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '927d8d22d8c7', 'cluster_name': 'docker-cluster', 'cluster_uuid': '038uq0CNScCWYvn2HwLEoA', 'version': {'number': '7.5.2', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '8bec50e1e0ad29dad5653712cf3bb580cd1afcdf', 'build_date': '2020-01-15T12:11:52.313576Z', 'build_snapshot': False, 'lucene_version': '8.3.0', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch('elasticsearch-node:9200')\n",
    "\n",
    "if es.indices.exists('stream-test'):\n",
    "    es.indices.delete('stream-test')\n",
    "    \n",
    "    body={\n",
    "        'mappings': {\n",
    "            'properties': {\n",
    "                'count': {'type': 'text'},\n",
    "                'name': {'type': 'text'},\n",
    "                'value': {'type': 'text'},\n",
    "                'timestamp': {'type': 'text'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    es.indices.create(index='stream-test', body=body)\n",
    "    \n",
    "print(es.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars elasticsearch-hadoop-7.5.2/dist/elasticsearch-spark-20_2.11-7.5.2.jar pyspark-shell'\n",
    "sc = SparkContext(appName=\"PythonSparkStreaming\")\n",
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan\n",
      "{\"count\": 5, \"name\": \"Bilbo\", \"value\": 37, \"timestamp\": 1581297174}\n",
      "5\n",
      "{\"count\": 3, \"name\": \"Bilbo\", \"value\": 66, \"timestamp\": 1581297172}\n",
      "3\n",
      "{\"count\": 4, \"name\": \"Legolas\", \"value\": 52, \"timestamp\": 1581297173}\n",
      "4\n",
      "{\"count\": 11, \"name\": \"Frodo\", \"value\": 4, \"timestamp\": 1581297180}\n",
      "11\n",
      "{\"count\": 7, \"name\": \"Legolas\", \"value\": 56, \"timestamp\": 1581297176}\n",
      "7\n",
      "{\"count\": 8, \"name\": \"Aragorn\", \"value\": 78, \"timestamp\": 1581297177}\n",
      "8\n",
      "{\"count\": 0, \"name\": \"Legolas\", \"value\": 54, \"timestamp\": 1581297169}\n",
      "0\n",
      "{\"count\": 6, \"name\": \"Samwise\", \"value\": 54, \"timestamp\": 1581297175}\n",
      "6\n",
      "{\"count\": 9, \"name\": \"Frodo\", \"value\": 91, \"timestamp\": 1581297178}\n",
      "9\n",
      "{\"count\": 2, \"name\": \"Bilbo\", \"value\": 79, \"timestamp\": 1581297171}\n",
      "2\n",
      "{\"count\": 12, \"name\": \"Legolas\", \"value\": 57, \"timestamp\": 1581297181}\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "json_folder_path = ('./sample/')\n",
    "\n",
    "json_folder_path = ('./sample/')\n",
    "json_files = [ x for x in os.listdir(json_folder_path ) if x.endswith(\"json\") ]\n",
    "json_merge = {}\n",
    "\n",
    "for json_file in json_files:\n",
    "\n",
    "    json_file_path = os.path.join(json_folder_path, json_file)\n",
    "\n",
    "    with open (json_file_path) as f:\n",
    "\n",
    "        data_dict = json.loads(f.read())  \n",
    "        json_merge = json.dumps(data_dict)\n",
    "        json_merge = str(json_merge)\n",
    "        #json_merge.update(data_dict)   \n",
    "        \n",
    "        #print(json_file)\n",
    "        #print(json_merge['count'])\n",
    "        #teste = json.dumps(data_dict, indent=4)\n",
    "        data_dict['count'] = data_dict.pop('count')\n",
    "        print(json_merge)\n",
    "        print(data_dict['count'])\n",
    "        \n",
    "        #-----------------------------------------------------------\n",
    "        \n",
    "        #'''\n",
    "        stream = ssc.textFileStream(json_file_path)\n",
    "        \n",
    "        es_write_conf = {\n",
    "            \"es.nodes\" : '927d8d22d8c7',\n",
    "            \"es.port\" : '9200',\n",
    "            \"es.resource\" : 'stream-test',\n",
    "            \"es.input.json\" : \"yes\",\n",
    "            \"es.mapping.id\": \"count\"\n",
    "        }\n",
    "        \n",
    "        rdd = sc.parallelize(data_dict)\n",
    "        \n",
    "        def format_data(x):\n",
    "            return (data_dict['count'], json.dumps(data_dict))      \n",
    "\n",
    "        # Uma função lambda pode receber qualquer número de argumentos, \n",
    "        # mas pode ter apenas uma expressão.\n",
    "\n",
    "        rdd = rdd.map(lambda x: format_data(x))\n",
    "        \n",
    "        parsed = stream.map(lambda x: format_data(x))\n",
    "        \n",
    "        rdd.saveAsNewAPIHadoopFile(\n",
    "            path='-',\n",
    "            outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n",
    "            keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "            valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "            conf=es_write_conf\n",
    "        )\n",
    "        \n",
    "        #'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stream = ssc.textFileStream(json_file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nes_write_conf = {\\n    \"es.nodes\" : \\'927d8d22d8c7\\',\\n    \"es.port\" : \\'9200\\',\\n    \"es.resource\" : \\'stream-test\\',\\n    \"es.input.json\" : \"yes\",\\n    \"es.mapping.id\": \"count\"\\n}\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "es_write_conf = {\n",
    "    \"es.nodes\" : '927d8d22d8c7',\n",
    "    \"es.port\" : '9200',\n",
    "    \"es.resource\" : 'stream-test',\n",
    "    \"es.input.json\" : \"yes\",\n",
    "    \"es.mapping.id\": \"count\"\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrdd = sc.parallelize(json_merge)\\n\\nprint(json_merge)\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "rdd = sc.parallelize(json_merge)\n",
    "\n",
    "print(json_merge)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef format_data(x):\\n    return (json_merge['count'], json.dumps(json_merge))      \\n\\n# Uma função lambda pode receber qualquer número de argumentos, \\n# mas pode ter apenas uma expressão.\\n\\nrdd = rdd.map(lambda x: format_data(x))\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def format_data(x):\n",
    "    return (json_merge['count'], json.dumps(json_merge))      \n",
    "\n",
    "# Uma função lambda pode receber qualquer número de argumentos, \n",
    "# mas pode ter apenas uma expressão.\n",
    "\n",
    "rdd = rdd.map(lambda x: format_data(x))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parsed = stream.map(lambda x: format_data(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrdd.saveAsNewAPIHadoopFile(\\n    path=\\'-\\',\\n    outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\\n    keyClass=\"org.apache.hadoop.io.NullWritable\",\\n    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\\n    conf=es_write_conf\\n)\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "rdd.saveAsNewAPIHadoopFile(\n",
    "    path='-',\n",
    "    outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "    conf=es_write_conf\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parsed.foreachRDD(lambda rdd: handler(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2020-02-14 01:32:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-02-14 01:32:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-02-14 01:32:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-02-14 01:32:57\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Bulk Processing ES with Spark</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"PythonSparkReading\")  \n",
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_read_conf = {\n",
    "    \"es.nodes\" : '927d8d22d8c7',\n",
    "    \"es.port\" : '9200',\n",
    "    \"es.resource\" : 'stream-test'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_rdd = sc.newAPIHadoopRDD(\n",
    "    inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "    conf=es_read_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4',\n",
       "  {'name': 'Legolas', 'count': '4', 'value': '52', 'timestamp': '1581297173'}),\n",
       " ('8',\n",
       "  {'name': 'Aragorn', 'count': '8', 'value': '78', 'timestamp': '1581297177'}),\n",
       " ('0',\n",
       "  {'name': 'Legolas', 'count': '0', 'value': '54', 'timestamp': '1581297169'}),\n",
       " ('6',\n",
       "  {'name': 'Samwise', 'count': '6', 'value': '54', 'timestamp': '1581297175'}),\n",
       " ('7',\n",
       "  {'name': 'Legolas', 'count': '7', 'value': '56', 'timestamp': '1581297176'})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_rdd = es_rdd.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Legolas', 'count': '4', 'value': '52', 'timestamp': '1581297173'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark SQL\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = es_rdd.map(lambda l: Row(**dict(l))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count='4', name='Legolas', timestamp='1581297173', value='52')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='54', count=2),\n",
       " Row(value='52', count=1),\n",
       " Row(value='78', count=1),\n",
       " Row(value='79', count=1),\n",
       " Row(value='56', count=1),\n",
       " Row(value='37', count=1),\n",
       " Row(value='4', count=1),\n",
       " Row(value='66', count=1),\n",
       " Row(value='91', count=1),\n",
       " Row(value='57', count=1)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df \\\n",
    "    .groupby('value') \\\n",
    "    .count() \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count='4', name='Legolas', timestamp='1581297173', value='52'),\n",
       " Row(count='0', name='Legolas', timestamp='1581297169', value='54'),\n",
       " Row(count='7', name='Legolas', timestamp='1581297176', value='56'),\n",
       " Row(count='12', name='Legolas', timestamp='1581297181', value='57')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df \\\n",
    "    .filter(df.name == 'Legolas')\\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
